{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb550b1-8359-42e5-80d4-73fc8d1bf096",
   "metadata": {},
   "source": [
    "# Using `bouquet` for NLP Classification with XGBoost\n",
    "Demonstrating how to use the `bouquet` framework to build a classifier that predicts Newsgroups from news clippings (text). <br>\n",
    "Link to dataset origin: [home page for newsgroups](http://qwone.com/~jason/20Newsgroups/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560786dc-4d21-41cb-bb1d-3f27184d5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.inference_model import XGBoostModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71843b5a-1d60-42da-a529-2ee29e663cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "# assert len(logger.handlers) == 1\n",
    "# handler = logger.handlers[0]\n",
    "# handler.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dfba71-3b26-4063-be8e-26d1cc753934",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset creation\n",
    "Unzip the files and collate the text files into one DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf0ecb3b-1723-4c4f-a3ab-7188e426e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip newsgroups dataset\n",
    "local_origin = \"../data/20news-bydate.tar.gz\"\n",
    "local_destination = \"../data/\"\n",
    "\n",
    "with tarfile.open(local_origin, \"r:gz\") as tar:\n",
    "    tar.extractall(local_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb833444-9339-482c-ac02-8ecd39b9c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in newsgroups dataset\n",
    "def collate_text_files(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads all text files located in subfolders of a directory and stores them\n",
    "    into a dataframe.\n",
    "    \"\"\"\n",
    "    files = [os.path.join(dirpath, f) \n",
    "             for (dirpath, dirnames, filenames) in os.walk(data_path)\n",
    "             for f in filenames]\n",
    "    texts = []\n",
    "    \n",
    "    for filepath in files:\n",
    "        try:\n",
    "            with open(filepath, \"r\") as f:\n",
    "                texts.append(f.read())\n",
    "        except UnicodeDecodeError:\n",
    "            texts.append(\"\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"file_path\": files,\n",
    "        \"text\": texts\n",
    "    })\n",
    "    print(df.shape)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c4ce725-3e7a-40c5-bfcc-4bfb52f77edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = collate_text_files(\"../data/20news-bydate-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "897fced7-dd5a-43d7-b7a1-04decd1c7985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 2)\n",
      "(11270, 2)\n"
     ]
    }
   ],
   "source": [
    "# check how many extractions failed\n",
    "print(train_df.loc[train_df[\"text\"] == \"\"].shape)\n",
    "\n",
    "# and drop those records\n",
    "train_df = train_df.loc[train_df[\"text\"] != \"\"].reset_index(drop=True)\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d37aab35-32f8-4dd1-b866-efc2a95be65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7532, 2)\n",
      "(7503, 2)\n"
     ]
    }
   ],
   "source": [
    "# do the same for test files\n",
    "test_df = collate_text_files(\"../data/20news-bydate-test\")\n",
    "test_df = test_df.loc[test_df[\"text\"] != \"\"].reset_index(drop=True)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31debc39-2eb5-4035-8f57-102c787c6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target class column\n",
    "train_df[\"newsgroup\"] = train_df[\"file_path\"].apply(lambda x: x.split(\"/\")[3])\n",
    "test_df[\"newsgroup\"] = test_df[\"file_path\"].apply(lambda x: x.split(\"/\")[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c7a4c3b-2867-4e9e-8d06-5814fde4b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets for easier loading next time\n",
    "train_df.to_csv(\"../data/20news_bydate_train.csv\", index=False)\n",
    "test_df.to_csv(\"../data/20news_bydate_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23451f46-db72-44d1-b70a-be9f41ca6d08",
   "metadata": {},
   "source": [
    "### Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87c2554e-d9d4-444b-b49d-62b022fe7efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newsgroup\n",
       "alt.atheism                 475\n",
       "comp.graphics               579\n",
       "comp.os.ms-windows.misc     591\n",
       "comp.sys.ibm.pc.hardware    588\n",
       "comp.sys.mac.hardware       567\n",
       "comp.windows.x              590\n",
       "misc.forsale                583\n",
       "rec.autos                   593\n",
       "rec.motorcycles             598\n",
       "rec.sport.baseball          591\n",
       "rec.sport.hockey            600\n",
       "sci.crypt                   594\n",
       "sci.electronics             588\n",
       "sci.med                     592\n",
       "sci.space                   593\n",
       "soc.religion.christian      599\n",
       "talk.politics.guns          546\n",
       "talk.politics.mideast       564\n",
       "talk.politics.misc          465\n",
       "talk.religion.misc          374\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(\"newsgroup\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e1098c-06c2-4e1b-8374-f8db57c69adb",
   "metadata": {},
   "source": [
    "The dataset is pretty balanced between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b945c-375f-4c8f-8208-df9ad6327883",
   "metadata": {},
   "source": [
    "## XGBoost Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "050e5f5d-3847-494c-83f8-127f482bb8cc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/sarah/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/sarah/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /Users/sarah/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/sarah/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c54be78f-1da0-41f1-a459-ce1560fbc60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    \"seed\": 13,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"gamma\": 0,\n",
    "    \"max_depth\": 5,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"lambda\": 1,\n",
    "    \"alpha\": 0,\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"multi_strategy\": \"one_output_per_tree\",\n",
    "    \"eval_metric\": [\"merror\", \"mlogloss\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54e4a810-3ff1-4c78-8b8a-5428dfec7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBoostModel(data_path=\"~/Documents/mission-control/bouquet/data/20news-bydate\",\n",
    "                         target=\"newsgroup\",\n",
    "                         xgb_kwargs=model_kwargs,\n",
    "                         max_features=10000,\n",
    "                         save_path=\"~/Documents/mission-control/bouquet/artifacts/20news-bydate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e83157bb-1dbc-4a98-b0a6-6734095d20ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inference_model:Cleaning text\n",
      "INFO:inference_model:Vectorizing text\n",
      "INFO:inference_model:Text cleaned and vectorized in 46.02586007118225 seconds\n",
      "INFO:inference_model:Encoding class labels\n"
     ]
    }
   ],
   "source": [
    "xgb_model.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f09b8da-8b7a-4d6e-bf44-4aabbe5a017a",
   "metadata": {},
   "source": [
    "Results aren't bad - 0.75 macro F1 - but could be way better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
